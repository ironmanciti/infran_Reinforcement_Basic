{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTfA3OUb3Hcx"
   },
   "source": [
    "# 110. Deep Neural Network을 이용한 함수 근사에서 필요한 torch basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXD5j_4_3Hc0",
    "outputId": "e30cc313-8249-44fa-dc7b-f5cd0520f953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import collections\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "env = gym.make('CartPole-v1')  \n",
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uciS0nsN3Hc2"
   },
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hyL29xvt3Hc3"
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "\n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mBA3O2s3Hc4",
    "outputId": "e298887e-22c3-4f9c-a871-83c697938130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.03732503, -0.03825209, -0.03765024, -0.00319418], dtype=float32),\n",
       "  1,\n",
       "  array([ 0.03655998,  0.15738903, -0.03771412, -0.30751443], dtype=float32),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([ 0.03655998,  0.15738903, -0.03771412, -0.30751443], dtype=float32),\n",
       "  0,\n",
       "  array([ 0.03970776, -0.0371758 , -0.04386441, -0.02696005], dtype=float32),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([ 0.03970776, -0.0371758 , -0.04386441, -0.02696005], dtype=float32),\n",
       "  1,\n",
       "  array([ 0.03896425,  0.15854685, -0.04440361, -0.33315364], dtype=float32),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([ 0.03896425,  0.15854685, -0.04440361, -0.33315364], dtype=float32),\n",
       "  0,\n",
       "  array([ 0.04213519, -0.03591589, -0.05106669, -0.05479741], dtype=float32),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([ 0.04213519, -0.03591589, -0.05106669, -0.05479741], dtype=float32),\n",
       "  1,\n",
       "  array([ 0.04141687,  0.15989968, -0.05216263, -0.36314493], dtype=float32),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([ 0.04141687,  0.15989968, -0.05216263, -0.36314493], dtype=float32),\n",
       "  1,\n",
       "  array([ 0.04461486,  0.3557227 , -0.05942553, -0.6718088 ], dtype=float32),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([ 0.04461486,  0.3557227 , -0.05942553, -0.6718088 ], dtype=float32),\n",
       "  0,\n",
       "  array([ 0.05172932,  0.16147497, -0.07286171, -0.39841238], dtype=float32),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([ 0.05172932,  0.16147497, -0.07286171, -0.39841238], dtype=float32),\n",
       "  1,\n",
       "  array([ 0.05495882,  0.35755086, -0.08082996, -0.71314824], dtype=float32),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([ 0.05495882,  0.35755086, -0.08082996, -0.71314824], dtype=float32),\n",
       "  0,\n",
       "  array([ 0.06210983,  0.16363554, -0.09509292, -0.44696307], dtype=float32),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([ 0.06210983,  0.16363554, -0.09509292, -0.44696307], dtype=float32),\n",
       "  0,\n",
       "  array([ 0.06538255, -0.0300216 , -0.10403218, -0.18570496], dtype=float32),\n",
       "  1.0,\n",
       "  False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize replay memory D to capacity N\n",
    "D = ExperienceReplay(10)\n",
    "\n",
    "s, _ = env.reset()\n",
    "for i in range(10):\n",
    "    a = env.action_space.sample()\n",
    "    s_, r, truncated, terminated, _ = env.step(a)\n",
    "    done = truncated or terminated\n",
    "    D.push(s, a, s_, r, done)\n",
    "    s = s_\n",
    "    \n",
    "D.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBWTiu01BTOB"
   },
   "source": [
    "## Sample random minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8t4tHzM6BO4_",
    "outputId": "74d7419c-16bd-4254-8ded-71c8b88b8f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------states-------------------------\n",
      "(array([ 0.04213519, -0.03591589, -0.05106669, -0.05479741], dtype=float32), array([ 0.05172932,  0.16147497, -0.07286171, -0.39841238], dtype=float32), array([ 0.04141687,  0.15989968, -0.05216263, -0.36314493], dtype=float32), array([ 0.03970776, -0.0371758 , -0.04386441, -0.02696005], dtype=float32), array([ 0.03732503, -0.03825209, -0.03765024, -0.00319418], dtype=float32))\n",
      "-------------actions----------------------\n",
      "(1, 1, 1, 1, 1)\n",
      "------------rewards------------------------\n",
      "(array([ 0.04141687,  0.15989968, -0.05216263, -0.36314493], dtype=float32), array([ 0.05495882,  0.35755086, -0.08082996, -0.71314824], dtype=float32), array([ 0.04461486,  0.3557227 , -0.05942553, -0.6718088 ], dtype=float32), array([ 0.03896425,  0.15854685, -0.04440361, -0.33315364], dtype=float32), array([ 0.03655998,  0.15738903, -0.03771412, -0.30751443], dtype=float32))\n",
      "------------next states--------------------\n",
      "(False, False, False, False, False)\n",
      "---------------dones-------------------------\n",
      "(1.0, 1.0, 1.0, 1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "states, actions, rewards, dones, next_states = D.sample(5)\n",
    "\n",
    "print(\"-------------states-------------------------\")\n",
    "print(states)\n",
    "print(\"-------------actions----------------------\")\n",
    "print(actions)\n",
    "print(\"------------rewards------------------------\")\n",
    "print(rewards)\n",
    "print(\"------------next states--------------------\")\n",
    "print(next_states)\n",
    "print(\"---------------dones-------------------------\")\n",
    "print(dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-xKKrqw3Hc5"
   },
   "source": [
    "## Select Action \n",
    "\n",
    "- state가 4 개의 feature로 구성되고 각 state에서의 action이 2 가지인 MDP의 parameter화 된 state action value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gbVFe-jz3Hc6"
   },
   "outputs": [],
   "source": [
    "n_inputs = 4  # state feature\n",
    "n_outputs = 2  # action space\n",
    "hidden_layer = 64\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = torch.relu(self.linear1(x))\n",
    "        output = self.linear2(a1)\n",
    "        return output\n",
    "\n",
    "Q = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0fir5Pl3Hc7"
   },
   "source": [
    "- 입력 : 4 개 feature 로 구성된 state \n",
    "- 출력 : 2 개 action values  \n",
    "\n",
    "- greedy action : $max_{a'}Q(s', a';\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQ1VsEkd3Hc7",
    "outputId": "c08766e7-27a2-4d04-f399-c3fd1d5e0584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1466, -0.1327], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "\n",
    "action_values = Q(torch.tensor(s).to(device))\n",
    "action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTXrKPAk3Hc8",
    "outputId": "ac2fd21b-945a-4ce1-f18c-31ecb8971e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greedy action\n",
    "action = torch.argmax(action_values).item() \n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dILVsp1p3Hc9"
   },
   "source": [
    "## State-Action Value (q value) from DQN \n",
    "\n",
    "Q-network 에서 입력으로 주어진 states 에 대응하는 action values 를 출력으로 얻어 greedy action 을 선택하는 code.  \n",
    "\n",
    "함수 max()는 최대값과 해당 값의 인덱스를 모두 반환하므로 최대값과 argmax를 모두 계산합니다. 이 경우 값에만 관심이 있기 때문에 결과의 첫 번째 항목(values)을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, new_states, rewards, dones = D.sample(5)\n",
    "\n",
    "states = torch.Tensor(states).to(device)\n",
    "actions = torch.LongTensor(actions).to(device)\n",
    "new_states = torch.Tensor(new_states).to(device)\n",
    "rewards = torch.Tensor([rewards]).to(device)\n",
    "dones = torch.Tensor(dones).to(device)\n",
    "\n",
    "new_action_values = Q(new_states).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.1617, 1.1517, 1.1485, 1.1598, 1.1576]]),\n",
       " tensor([[ 0.1640],\n",
       "         [-0.1431],\n",
       "         [ 0.1517],\n",
       "         [ 0.1573],\n",
       "         [-0.1087]], grad_fn=<GatherBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 1.0\n",
    "y_target = rewards + \\\n",
    "                (1 - dones) * GAMMA * torch.max(new_action_values, 1)[0]\n",
    "y_pred = Q(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "y_target, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSugiYLx3Hc-"
   },
   "source": [
    "## torch.gather\n",
    "\n",
    "- torch.gather 함수 (또는 torch.Tensor.gather)는 다중 인덱스 선택 방법  \n",
    "\n",
    "- 첫 번째 인수인 input은 요소를 선택하려는 소스 텐서. 두 번째 dim은 수집하려는 차원. 마지막으로 index는 입력을 인덱싱하는 인덱스.\n",
    "\n",
    "4개의 항목과 4개의 작업으로 구성된 일괄 처리가 있는 간단한 예제 사례에서 gather가 수행하는 작업의 요약입니다.\n",
    "\n",
    "```\n",
    "state_action_values = net(states_v).gather(1, actions_v.unsqueeze(1))\n",
    "```\n",
    "\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/1*fS-9p5EBKVgl69Gy0gwjGQ.png width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        self.fc2 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.softmax(self.fc2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = PolicyNetwork(input_dims=env.observation_space.shape,\n",
    "                   n_actions=env.action_space.n).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsSGQnQa3Hc_",
    "outputId": "9c50f027-6290-4904-8017-ee63234e88aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0517,  0.1615, -0.0729, -0.3984],\n",
       "        [ 0.0414,  0.1599, -0.0522, -0.3631],\n",
       "        [ 0.0621,  0.1636, -0.0951, -0.4470],\n",
       "        [ 0.0550,  0.3576, -0.0808, -0.7131],\n",
       "        [ 0.0390,  0.1585, -0.0444, -0.3332]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_v  # 4개의 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6098, -0.7841],\n",
       "        [-0.6090, -0.7851],\n",
       "        [-0.6127, -0.7806],\n",
       "        [-0.6284, -0.7624],\n",
       "        [-0.6078, -0.7864]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob = torch.log(pi(states_v))\n",
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6098, -0.7851, -0.6127, -0.6284, -0.7864]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_log_probs = rewards * \\\n",
    "                        torch.gather(log_prob, 1, actions.unsqueeze(1)).squeeze()\n",
    "selected_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufB1JYnC3HdB"
   },
   "source": [
    "## REINFORECE 알고리즘 지원을 위한 PROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS\n",
    "\n",
    "- distribution 패키지에는 매개변수화할 수 있는 확률 분포와 sampling 함수가 포함되어 있습니다. 이를 통해 최적화를 위한 확률적 계산 그래프 및 확률적 기울기 추정기를 구성할 수 있습니다. \n",
    "\n",
    "- torch 는 다음과 같이 REINFORCE 알고리즘을 지원합니다.\n",
    "\n",
    "```python\n",
    "    probs = policy_network(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    next_state, reward = env.step(action)\n",
    "    loss = -m.log_prob(action) * reward\n",
    "    loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEN8t3cq3HdB"
   },
   "source": [
    "### 방법 1) Categorical(probs) 에서 sampling\n",
    "\n",
    "'probs'가 길이가 'K'인 1차원 array인 경우, 각 element 는 해당 인덱스에서 클래스를 샘플링할 상대 확률입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRFyebJE3HdB",
    "outputId": "98223f29-3e0b-4dc3-ecc5-682cb2f477dd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "logits = torch.rand(4)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(f\"softmax 확률 분포 : {probs}, sum = {probs.sum()}\")\n",
    "\n",
    "# 각 class 를 sampling 할 상대 확률\n",
    "m = Categorical(probs)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TACqKr0Y3HdC"
   },
   "source": [
    "위의 m 에서 sampling 을 반복하면 softmax 확률 분포로 sampling 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHvhhzZO3HdC",
    "outputId": "3e2e55c8-868a-4315-e11d-c5cfce92dc4b"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "samples = []\n",
    "\n",
    "for _ in range(30000):\n",
    "    a = m.sample()\n",
    "    samples.append(a.item())\n",
    "\n",
    "[cnt/len(samples) for a, cnt in sorted(Counter(samples).items())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muu7DisY3HdC"
   },
   "source": [
    "### 방법 2) np.random.choice 에서 sampling\n",
    "\n",
    "- np.random.choice 의 `parameter p`에 softmax 확률 분포 지정하여 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewhGZw5w3HdC",
    "outputId": "bc228a97-4870-4343-cae7-645ae0bff82e"
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for _ in range(30000):\n",
    "    a = np.random.choice(4, p=probs.numpy())\n",
    "    samples.append(a)\n",
    "    \n",
    "[cnt/len(samples) for a, cnt in sorted(Counter(samples).items())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13-ePWwd3HdD"
   },
   "source": [
    "### REINFORCE 구현을  위해  total expected return $G_t$ 를 estimate 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA4SgEC23HdD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 5 step 만에 spisode 종료 가정\n",
    "rewards = [1, 2, 3, 4, 5]\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgyPm6P-3HdD",
    "outputId": "5a2c6d26-e9c2-46a1-9d8e-70ce7caad763"
   },
   "outputs": [],
   "source": [
    "G_0 = 1 + 0.99**1 * 2 + 0.99**2 * 3 + 0.99**3 * 4 + 0.99**4 * 5\n",
    "G_1 = 2 + 0.99**1 * 3 + 0.99**2 * 4 + 0.99**3 * 5\n",
    "G_2 = 3 + 0.99**1 * 4 + 0.99**2 * 5\n",
    "G_3 = 4 + 0.99**1 * 5\n",
    "G_4 = 5\n",
    "print(G_0, G_1, G_2, G_3, G_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcFvCy3I3HdD"
   },
   "outputs": [],
   "source": [
    "r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
    "# Reverse the array direction for cumsum and then\n",
    "# revert back to the original order\n",
    "r = r[::-1].cumsum()[::-1]\n",
    "# return r - r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkFx4vW13HdD",
    "outputId": "e5fff025-cbe8-4d41-a3a7-e25e12393d9a"
   },
   "outputs": [],
   "source": [
    "# episodic task\n",
    "Returns = []\n",
    "G = 0\n",
    "for r in rewards[::-1]:\n",
    "    G = r + gamma * G\n",
    "    Returns.append(G)\n",
    "    \n",
    "Returns = np.array(Returns[::-1], dtype=np.float64)\n",
    "Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlrV5LBC3HdE",
    "outputId": "f4289481-6034-4738-f441-fdb900a3a6e7"
   },
   "outputs": [],
   "source": [
    "# continuing task\n",
    "def discount_rewards(rewards):\n",
    "    Returns = []\n",
    "    G = 0\n",
    "    for r in rewards[::-1]:\n",
    "        G = r + gamma * G\n",
    "        Returns.append(G)\n",
    "    # cumsum의 배열 방향을 반대로 한 다음 원래 순서로 되돌립니다.\n",
    "    Returns = np.array(Returns[::-1], dtype=np.float64)\n",
    "    print(Returns)\n",
    "    return Returns - Returns.mean()\n",
    "\n",
    "discount_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "simlDDmD3HdE"
   },
   "source": [
    "### REINFORCE 구현을 위한 Score Function\n",
    "\n",
    "- 확률 밀도 함수가 매개 변수와 관련하여 미분할 수있는 경우 REINFORCE를 구현하려면 sample () 및 log_prob () 만 필요\n",
    "\n",
    "$$\\Delta_{\\theta} = \\alpha r \\frac{\\partial log p(a | \\pi^{\\theta}(s))}{\\partial\\theta}$$  \n",
    "\n",
    "$\\alpha$ - learning rate, r - reward,  $p(a|\\pi^\\theta(s))$ - probability of taking action a  \n",
    "\n",
    "\n",
    "- Network 출력에서 action을 샘플링하고 이 action을 environment에 적용한 다음 log_prob를 사용하여 동등한 손실 함수를 구성.   \n",
    "- optimizer는 경사 하강법을 사용하기 때문에 음수를 사용하는 반면 위의 규칙은 경사 상승을 가정.   \n",
    "- Categorical Policy를 사용하는 경우 REINFORCE를 구현하는 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhVDiCjk3HdE",
    "outputId": "77da07fc-a3f6-4897-9e19-3f837cf924a8"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')  \n",
    "s = env.reset()\n",
    "\n",
    "#probs = policy_network(state)\n",
    "logits = torch.rand(2)\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "m = Categorical(probs)\n",
    "action = m.sample()\n",
    "\n",
    "next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "loss = -m.log_prob(action) * reward\n",
    "#loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZhEg-H53HdE"
   },
   "source": [
    "## Huber Loss\n",
    "\n",
    "- Actor-Critic 의 critic value function 의 loss 계산에 사용  \n",
    "- Huber Loss는 L1과 L2의 장점을 취하면서 단점을 보완하기 위해서 제안된 것이 Huber Loss다.\n",
    "    - 모든 지점에서 미분이 가능하다.  \n",
    "    - Outlier에 상대적으로 Robust하다.\n",
    "<img src=https://bekaykang.github.io/assets/img/post/201209-2.png width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn8JMIvJ3HdF",
    "outputId": "64fbcd18-6ad3-4227-e2ba-3fd8d6eab8e0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "curr_q = torch.FloatTensor([10,11,12,10,9])\n",
    "target_q = torch.FloatTensor([12,8,10,13,11])\n",
    "\n",
    "loss = F.smooth_l1_loss(curr_q, target_q)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.mse_loss(curr_q, target_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "110_basic_operations_for_Function_Approximation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
