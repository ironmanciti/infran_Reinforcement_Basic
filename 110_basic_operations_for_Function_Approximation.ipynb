{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTfA3OUb3Hcx"
      },
      "source": [
        "# 110. Deep Neural Network을 이용한 함수 근사에서 필요한 torch basics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vOlHN2ueeFo",
        "outputId": "20b54888-5318-4aee-c874-fa4203bd6c3a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXD5j_4_3Hc0",
        "outputId": "10b3dcc8-3c5c-4291-9df9-815598bb84e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import collections\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "action_size = env.action_space.n\n",
        "action_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uciS0nsN3Hc2"
      },
      "source": [
        "## Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hyL29xvt3Hc3"
      },
      "outputs": [],
      "source": [
        "class ExperienceReplay:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, new_state, reward, done):\n",
        "        transition = (state, action, new_state, reward, done)\n",
        "\n",
        "        if self.position >= len(self.memory):\n",
        "            self.memory.append(transition)\n",
        "        else:\n",
        "            self.memory[self.position] = transition\n",
        "\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return zip(*random.sample(self.memory, batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mBA3O2s3Hc4",
        "outputId": "892cc034-0980-4544-df16-deba9ca2b3ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array([ 0.01018644, -0.02288333, -0.00339918, -0.04913813], dtype=float32),\n",
              "  1,\n",
              "  array([ 0.00972877,  0.1722872 , -0.00438194, -0.34289157], dtype=float32),\n",
              "  1.0,\n",
              "  False),\n",
              " (array([ 0.00972877,  0.1722872 , -0.00438194, -0.34289157], dtype=float32),\n",
              "  1,\n",
              "  array([ 0.01317452,  0.36747122, -0.01123978, -0.63695306], dtype=float32),\n",
              "  1.0,\n",
              "  False),\n",
              " (array([ 0.01317452,  0.36747122, -0.01123978, -0.63695306], dtype=float32),\n",
              "  1,\n",
              "  array([ 0.02052394,  0.5627481 , -0.02397884, -0.93315434], dtype=float32),\n",
              "  1.0,\n",
              "  False),\n",
              " (array([ 0.02052394,  0.5627481 , -0.02397884, -0.93315434], dtype=float32),\n",
              "  0,\n",
              "  array([ 0.0317789 ,  0.36795774, -0.04264193, -0.64810187], dtype=float32),\n",
              "  1.0,\n",
              "  False),\n",
              " (array([ 0.0317789 ,  0.36795774, -0.04264193, -0.64810187], dtype=float32),\n",
              "  0,\n",
              "  array([ 0.03913806,  0.17345497, -0.05560396, -0.3691459 ], dtype=float32),\n",
              "  1.0,\n",
              "  False),\n",
              " (array([ 0.03913806,  0.17345497, -0.05560396, -0.3691459 ], dtype=float32),\n",
              "  0,\n",
              "  array([ 0.04260715, -0.02083466, -0.06298688, -0.094501  ], dtype=float32),\n",
              "  1.0,\n",
              "  False),\n",
              " (array([ 0.04260715, -0.02083466, -0.06298688, -0.094501  ], dtype=float32),\n",
              "  0,\n",
              "  array([ 0.04219046, -0.21499993, -0.0648769 ,  0.17766346], dtype=float32),\n",
              "  1.0,\n",
              "  False),\n",
              " (array([ 0.04219046, -0.21499993, -0.0648769 ,  0.17766346], dtype=float32),\n",
              "  0,\n",
              "  array([ 0.03789046, -0.40913635, -0.06132363,  0.44919503], dtype=float32),\n",
              "  1.0,\n",
              "  False),\n",
              " (array([ 0.03789046, -0.40913635, -0.06132363,  0.44919503], dtype=float32),\n",
              "  0,\n",
              "  array([ 0.02970774, -0.60333973, -0.05233973,  0.7219347 ], dtype=float32),\n",
              "  1.0,\n",
              "  False),\n",
              " (array([ 0.02970774, -0.60333973, -0.05233973,  0.7219347 ], dtype=float32),\n",
              "  0,\n",
              "  array([ 0.01764094, -0.7977001 , -0.03790104,  0.99769515], dtype=float32),\n",
              "  1.0,\n",
              "  False)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#Initialize replay memory D to capacity N\n",
        "D = ExperienceReplay(10)\n",
        "\n",
        "s, _ = env.reset()\n",
        "for i in range(10):\n",
        "    a = env.action_space.sample()\n",
        "    s_, r, truncated, terminated, _ = env.step(a)\n",
        "    done = truncated or terminated\n",
        "    D.push(s, a, s_, r, done)\n",
        "    s = s_\n",
        "\n",
        "D.memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBWTiu01BTOB"
      },
      "source": [
        "## Sample random minibatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t4tHzM6BO4_",
        "outputId": "6db53817-6ed4-4163-ea79-15fd6b67a3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------states-------------------------\n",
            "(array([ 0.04260715, -0.02083466, -0.06298688, -0.094501  ], dtype=float32), array([ 0.0317789 ,  0.36795774, -0.04264193, -0.64810187], dtype=float32), array([ 0.03789046, -0.40913635, -0.06132363,  0.44919503], dtype=float32), array([ 0.01317452,  0.36747122, -0.01123978, -0.63695306], dtype=float32), array([ 0.00972877,  0.1722872 , -0.00438194, -0.34289157], dtype=float32))\n",
            "-------------actions----------------------\n",
            "(0, 0, 0, 1, 1)\n",
            "------------rewards------------------------\n",
            "(array([ 0.04219046, -0.21499993, -0.0648769 ,  0.17766346], dtype=float32), array([ 0.03913806,  0.17345497, -0.05560396, -0.3691459 ], dtype=float32), array([ 0.02970774, -0.60333973, -0.05233973,  0.7219347 ], dtype=float32), array([ 0.02052394,  0.5627481 , -0.02397884, -0.93315434], dtype=float32), array([ 0.01317452,  0.36747122, -0.01123978, -0.63695306], dtype=float32))\n",
            "------------next states--------------------\n",
            "(False, False, False, False, False)\n",
            "---------------dones-------------------------\n",
            "(1.0, 1.0, 1.0, 1.0, 1.0)\n"
          ]
        }
      ],
      "source": [
        "states, actions, rewards, dones, next_states = D.sample(5)\n",
        "\n",
        "print(\"-------------states-------------------------\")\n",
        "print(states)\n",
        "print(\"-------------actions----------------------\")\n",
        "print(actions)\n",
        "print(\"------------rewards------------------------\")\n",
        "print(rewards)\n",
        "print(\"------------next states--------------------\")\n",
        "print(next_states)\n",
        "print(\"---------------dones-------------------------\")\n",
        "print(dones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-xKKrqw3Hc5"
      },
      "source": [
        "## Select Action\n",
        "\n",
        "- state가 4 개의 feature로 구성되고 각 state에서의 action이 2 가지인 MDP의 parameter화 된 state action value function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gbVFe-jz3Hc6"
      },
      "outputs": [],
      "source": [
        "n_inputs = 4  # state feature\n",
        "n_outputs = 2  # action space\n",
        "hidden_layer = 64\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(n_inputs, hidden_layer)\n",
        "        self.linear2 = nn.Linear(hidden_layer, n_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a1 = torch.relu(self.linear1(x))\n",
        "        output = self.linear2(a1)\n",
        "        return output\n",
        "\n",
        "Q = NeuralNetwork().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fir5Pl3Hc7"
      },
      "source": [
        "- 입력 : 4 개 feature 로 구성된 state\n",
        "- 출력 : 2 개 action values  \n",
        "\n",
        "- greedy action : $max_{a'}Q(s', a';\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ1VsEkd3Hc7",
        "outputId": "2a6acabc-61ba-47e1-df22-939483757e00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0711, 0.0196], device='cuda:0', grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "s, _ = env.reset()\n",
        "\n",
        "action_values = Q(torch.tensor(s).to(device))\n",
        "action_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTXrKPAk3Hc8",
        "outputId": "295e141e-4e17-4924-8c33-45ecad71a0c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# greedy action\n",
        "action = torch.argmax(action_values).item()\n",
        "action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dILVsp1p3Hc9"
      },
      "source": [
        "## State-Action Value (q value) from DQN\n",
        "\n",
        "Q-network 에서 입력으로 주어진 states 에 대응하는 action values 를 출력으로 얻어 greedy action 을 선택하는 code.  \n",
        "\n",
        "함수 max()는 최대값과 해당 값의 인덱스를 모두 반환하므로 최대값과 argmax를 모두 계산합니다. 이 경우 값에만 관심이 있기 때문에 결과의 첫 번째 항목(values)을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4FU9vEseM0a",
        "outputId": "aa0e7a4e-e286-44a9-f9c4-f270c376623e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0596,  0.0033],\n",
            "        [-0.0719, -0.0386],\n",
            "        [ 0.1581, -0.0951],\n",
            "        [-0.0721, -0.0348],\n",
            "        [-0.0009, -0.0053]])\n",
            "torch.return_types.max(\n",
            "values=tensor([ 0.0596, -0.0386,  0.1581, -0.0348, -0.0009]),\n",
            "indices=tensor([0, 1, 0, 1, 0]))\n",
            "\n",
            "tensor([ 0.0596, -0.0386,  0.1581, -0.0348, -0.0009])\n",
            "tensor([0, 1, 0, 1, 0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-43047d261bb4>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  states_v = torch.tensor(states).to(device)\n"
          ]
        }
      ],
      "source": [
        "states_v = torch.tensor(states).to(device)\n",
        "action_values = Q(states_v).detach().cpu()\n",
        "\n",
        "print(action_values)\n",
        "print(torch.max(action_values, dim=1))\n",
        "print()\n",
        "\n",
        "values, indices = torch.max(action_values, dim=1)\n",
        "\n",
        "print(values)\n",
        "print(indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSugiYLx3Hc-"
      },
      "source": [
        "## torch.gather\n",
        "\n",
        "- torch.gather 함수 (또는 torch.Tensor.gather)는 다중 인덱스 선택 방법  \n",
        "\n",
        "- 첫 번째 인수인 input은 요소를 선택하려는 소스 텐서. 두 번째 dim은 수집하려는 차원. 마지막으로 index는 입력을 인덱싱하는 인덱스.\n",
        "\n",
        "4개의 항목과 4개의 작업으로 구성된 일괄 처리가 있는 간단한 예제 사례에서 gather가 수행하는 작업의 요약입니다.\n",
        "\n",
        "```\n",
        "state_action_values = net(states_v).gather(1, actions_v.unsqueeze(1))\n",
        "```\n",
        "\n",
        "\n",
        "<img src=https://miro.medium.com/max/1400/1*fS-9p5EBKVgl69Gy0gwjGQ.png width=400>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSGQnQa3Hc_",
        "outputId": "4fa4fdbf-3bed-471f-b72f-7ded9c99dbb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0426, -0.0208, -0.0630, -0.0945],\n",
              "        [ 0.0318,  0.3680, -0.0426, -0.6481],\n",
              "        [ 0.0379, -0.4091, -0.0613,  0.4492],\n",
              "        [ 0.0132,  0.3675, -0.0112, -0.6370],\n",
              "        [ 0.0097,  0.1723, -0.0044, -0.3429]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "states_v  # 4개의 feature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_values = Q(states_v)\n",
        "q_values  # 2 개의 action values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD-mRUsViTIq",
        "outputId": "9e7f7d7e-02d7-4c6e-8daa-3859e5a56195"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0596,  0.0033],\n",
              "        [-0.0719, -0.0386],\n",
              "        [ 0.1581, -0.0951],\n",
              "        [-0.0721, -0.0348],\n",
              "        [-0.0009, -0.0053]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action = torch.LongTensor([1, 0, 1, 1, 0]).unsqueeze(1).to(device)\n",
        "action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0iyPk9tiTEc",
        "outputId": "b57299fb-2438-4ad1-c366-43fd10b39d48"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [0],\n",
              "        [1],\n",
              "        [1],\n",
              "        [0]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.gather(q_values, 1, action)  #q_value의 axis=1에서 action index 수집"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWMhG-8ciS_o",
        "outputId": "31128e04-e473-487c-cc43-cd2ba5d095e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0033],\n",
              "        [-0.0719],\n",
              "        [-0.0951],\n",
              "        [-0.0348],\n",
              "        [-0.0009]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_values.gather(1, action)   # 위와 동일 operation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCwYsuNJiS3E",
        "outputId": "0ed632ef-ad62-472f-a0cc-86868a761dbf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0033],\n",
              "        [-0.0719],\n",
              "        [-0.0951],\n",
              "        [-0.0348],\n",
              "        [-0.0009]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufB1JYnC3HdB"
      },
      "source": [
        "## REINFORECE 알고리즘 지원을 위한 PROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS\n",
        "\n",
        "- distribution 패키지에는 매개변수화할 수 있는 확률 분포와 sampling 함수가 포함되어 있습니다. 이를 통해 최적화를 위한 확률적 계산 그래프 및 확률적 기울기 추정기를 구성할 수 있습니다.\n",
        "\n",
        "- torch 는 다음과 같이 REINFORCE 알고리즘을 지원합니다.\n",
        "\n",
        "```python\n",
        "    probs = policy_network(state)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    next_state, reward = env.step(action)\n",
        "    loss = -m.log_prob(action) * reward\n",
        "    loss.backward()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEN8t3cq3HdB"
      },
      "source": [
        "### 방법 1) Categorical(probs) 에서 sampling\n",
        "\n",
        "'probs'가 길이가 'K'인 1차원 array인 경우, 각 element 는 해당 인덱스에서 클래스를 샘플링할 상대 확률입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRFyebJE3HdB",
        "outputId": "bd783d39-ce75-4d50-fd6e-116d8bb0e6ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax 확률 분포 : tensor([0.3145, 0.3387, 0.1848, 0.1621]), sum = 1.0000001192092896\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Categorical(probs: torch.Size([4]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import torch\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "logits = torch.rand(4)\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "print(f\"softmax 확률 분포 : {probs}, sum = {probs.sum()}\")\n",
        "\n",
        "# 각 class 를 sampling 할 상대 확률\n",
        "m = Categorical(probs)\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TACqKr0Y3HdC"
      },
      "source": [
        "위의 m 에서 sampling 을 반복하면 softmax 확률 분포로 sampling 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHvhhzZO3HdC",
        "outputId": "e97b00d8-6b35-4f76-972c-76f475dd39ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3151333333333333, 0.3369, 0.1857, 0.16226666666666667]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from collections import Counter\n",
        "samples = []\n",
        "\n",
        "for _ in range(30000):\n",
        "    a = m.sample()\n",
        "    samples.append(a.item())\n",
        "\n",
        "[cnt/len(samples) for a, cnt in sorted(Counter(samples).items())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muu7DisY3HdC"
      },
      "source": [
        "### 방법 2) np.random.choice 에서 sampling\n",
        "\n",
        "- np.random.choice 의 `parameter p`에 softmax 확률 분포 지정하여 sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewhGZw5w3HdC",
        "outputId": "d88c385d-1342-45dc-89e9-89c1436f2c67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31456666666666666, 0.33723333333333333, 0.1886, 0.1596]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "samples = []\n",
        "\n",
        "for _ in range(30000):\n",
        "    a = np.random.choice(4, p=probs.numpy())\n",
        "    samples.append(a)\n",
        "\n",
        "[cnt/len(samples) for a, cnt in sorted(Counter(samples).items())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13-ePWwd3HdD"
      },
      "source": [
        "### REINFORCE 구현을  위해  total expected return $G_t$ 를 estimate 하는 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dA4SgEC23HdD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 5 step 만에 spisode 종료 가정\n",
        "rewards = [1, 2, 3, 4, 5]\n",
        "gamma = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgyPm6P-3HdD",
        "outputId": "8edb29fe-67fd-415a-8218-9a060f7efebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.604476049999999 13.741895 11.8605 8.95 5\n"
          ]
        }
      ],
      "source": [
        "G_0 = 1 + 0.99**1 * 2 + 0.99**2 * 3 + 0.99**3 * 4 + 0.99**4 * 5\n",
        "G_1 = 2 + 0.99**1 * 3 + 0.99**2 * 4 + 0.99**3 * 5\n",
        "G_2 = 3 + 0.99**1 * 4 + 0.99**2 * 5\n",
        "G_3 = 4 + 0.99**1 * 5\n",
        "G_4 = 5\n",
        "print(G_0, G_1, G_2, G_3, G_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fcFvCy3I3HdD"
      },
      "outputs": [],
      "source": [
        "r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
        "# Reverse the array direction for cumsum and then\n",
        "# revert back to the original order\n",
        "r = r[::-1].cumsum()[::-1]\n",
        "# return r - r.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkFx4vW13HdD",
        "outputId": "864d42fa-0fe7-42c8-a490-06db3d86eef3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14.60447605, 13.741895  , 11.8605    ,  8.95      ,  5.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# episodic task\n",
        "Returns = []\n",
        "G = 0\n",
        "for r in rewards[::-1]:\n",
        "    G = r + gamma * G\n",
        "    Returns.append(G)\n",
        "\n",
        "Returns = np.array(Returns[::-1], dtype=np.float64)\n",
        "Returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlrV5LBC3HdE",
        "outputId": "cd001a48-80d6-4fa8-94d1-4ce3ec69ced1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.60447605 13.741895   11.8605      8.95        5.        ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.77310184,  2.91052079,  1.02912579, -1.88137421, -5.83137421])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# continuing task\n",
        "def discount_rewards(rewards):\n",
        "    Returns = []\n",
        "    G = 0\n",
        "    for r in rewards[::-1]:\n",
        "        G = r + gamma * G\n",
        "        Returns.append(G)\n",
        "    # cumsum의 배열 방향을 반대로 한 다음 원래 순서로 되돌립니다.\n",
        "    Returns = np.array(Returns[::-1], dtype=np.float64)\n",
        "    print(Returns)\n",
        "    return Returns - Returns.mean()\n",
        "\n",
        "discount_rewards(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simlDDmD3HdE"
      },
      "source": [
        "### REINFORCE 구현을 위한 Score Function\n",
        "\n",
        "- 확률 밀도 함수가 매개 변수와 관련하여 미분할 수있는 경우 REINFORCE를 구현하려면 sample () 및 log_prob () 만 필요\n",
        "\n",
        "$$\\Delta_{\\theta} = \\alpha r \\frac{\\partial log p(a | \\pi^{\\theta}(s))}{\\partial\\theta}$$  \n",
        "\n",
        "$\\alpha$ - learning rate, r - reward,  $p(a|\\pi^\\theta(s))$ - probability of taking action a  \n",
        "\n",
        "\n",
        "- Network 출력에서 action을 샘플링하고 이 action을 environment에 적용한 다음 log_prob를 사용하여 동등한 손실 함수를 구성.   \n",
        "- optimizer는 경사 하강법을 사용하기 때문에 음수를 사용하는 반면 위의 규칙은 경사 상승을 가정.   \n",
        "- Categorical Policy를 사용하는 경우 REINFORCE를 구현하는 코드는 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhVDiCjk3HdE",
        "outputId": "06625fd9-5a2a-4e3a-bc9e-4f997b434f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7648)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "s = env.reset()\n",
        "\n",
        "#probs = policy_network(state)\n",
        "logits = torch.rand(2)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "m = Categorical(probs)\n",
        "action = m.sample()\n",
        "\n",
        "next_state, reward, done, _, _ = env.step(action.item())\n",
        "\n",
        "loss = -m.log_prob(action) * reward\n",
        "#loss.backward()\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZhEg-H53HdE"
      },
      "source": [
        "## Huber Loss\n",
        "\n",
        "- Actor-Critic 의 critic value function 의 loss 계산에 사용  \n",
        "- Huber Loss는 L1과 L2의 장점을 취하면서 단점을 보완하기 위해서 제안된 것이 Huber Loss다.\n",
        "    - 모든 지점에서 미분이 가능하다.  \n",
        "    - Outlier에 상대적으로 Robust하다.\n",
        "<img src=https://bekaykang.github.io/assets/img/post/201209-2.png width=300>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn8JMIvJ3HdF",
        "outputId": "76a5b0dd-e2ab-48ab-e5c5-cf74d4a49636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.9000)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "curr_q = torch.FloatTensor([10,11,12,10,9])\n",
        "target_q = torch.FloatTensor([12,8,10,13,11])\n",
        "\n",
        "loss = F.smooth_l1_loss(curr_q, target_q)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxtzHqRFeM0n",
        "outputId": "f548fe5e-e21c-476b-bbbf-0d5a68e263e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "F.mse_loss(curr_q, target_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "73w6HEA-eM0o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}