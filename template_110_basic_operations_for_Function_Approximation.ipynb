{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084fc017",
   "metadata": {
    "id": "yTfA3OUb3Hcx"
   },
   "source": [
    "# 110. Deep Neural Network을 이용한 함수 근사에서 필요한 torch basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a4072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a80042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78cfcaaa",
   "metadata": {
    "id": "uciS0nsN3Hc2"
   },
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce24cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "    def sample(self, batch_size):\n",
    "    def __len__(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce53747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize replay memory D to capacity N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4308f6",
   "metadata": {
    "id": "rBWTiu01BTOB"
   },
   "source": [
    "## Sample random minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3cf72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d309f324",
   "metadata": {
    "id": "c-xKKrqw3Hc5"
   },
   "source": [
    "## Select Action\n",
    "\n",
    "- state가 4 개의 feature로 구성되고 각 state에서의 action이 2 가지인 MDP의 parameter화 된 state action value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "    def forward(self, x):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e96949",
   "metadata": {
    "id": "d0fir5Pl3Hc7"
   },
   "source": [
    "- 입력 : 4 개 feature 로 구성된 state\n",
    "- 출력 : 2 개 action values  \n",
    "\n",
    "- greedy action : $max_{a'}Q(s', a';\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27709609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8337939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430c003",
   "metadata": {
    "id": "dILVsp1p3Hc9"
   },
   "source": [
    "## State-Action Value (q value) from DQN\n",
    "\n",
    "Q-network 에서 입력으로 주어진 states 에 대응하는 action values 를 출력으로 얻어 greedy action 을 선택하는 code.  \n",
    "\n",
    "함수 max()는 최대값과 해당 값의 인덱스를 모두 반환하므로 최대값과 argmax를 모두 계산합니다. 이 경우 값에만 관심이 있기 때문에 결과의 첫 번째 항목(values)을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6132cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b72d0e5",
   "metadata": {
    "id": "oSugiYLx3Hc-"
   },
   "source": [
    "## torch.gather\n",
    "\n",
    "- torch.gather 함수 (또는 torch.Tensor.gather)는 다중 인덱스 선택 방법  \n",
    "\n",
    "- 첫 번째 인수인 input은 요소를 선택하려는 소스 텐서. 두 번째 dim은 수집하려는 차원. 마지막으로 index는 입력을 인덱싱하는 인덱스.\n",
    "\n",
    "4개의 항목과 4개의 작업으로 구성된 일괄 처리가 있는 간단한 예제 사례에서 gather가 수행하는 작업의 요약입니다.\n",
    "\n",
    "```\n",
    "state_action_values = net(states_v).gather(1, actions_v.unsqueeze(1))\n",
    "```\n",
    "\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/1*fS-9p5EBKVgl69Gy0gwjGQ.png width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e8c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8fff38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7860c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee25055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf44a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ba884fc",
   "metadata": {
    "id": "ufB1JYnC3HdB"
   },
   "source": [
    "## REINFORECE 알고리즘 지원을 위한 PROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS\n",
    "\n",
    "- distribution 패키지에는 매개변수화할 수 있는 확률 분포와 sampling 함수가 포함되어 있습니다. 이를 통해 최적화를 위한 확률적 계산 그래프 및 확률적 기울기 추정기를 구성할 수 있습니다.\n",
    "\n",
    "- torch 는 다음과 같이 REINFORCE 알고리즘을 지원합니다.\n",
    "\n",
    "```python\n",
    "    probs = policy_network(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    next_state, reward = env.step(action)\n",
    "    loss = -m.log_prob(action) * reward\n",
    "    loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c3a7c",
   "metadata": {
    "id": "hEN8t3cq3HdB"
   },
   "source": [
    "### 방법 1) Categorical(probs) 에서 sampling\n",
    "\n",
    "'probs'가 길이가 'K'인 1차원 array인 경우, 각 element 는 해당 인덱스에서 클래스를 샘플링할 상대 확률입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95404f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 class 를 sampling 할 상대 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5fa80",
   "metadata": {
    "id": "TACqKr0Y3HdC"
   },
   "source": [
    "위의 m 에서 sampling 을 반복하면 softmax 확률 분포로 sampling 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f2d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9618cf1",
   "metadata": {
    "id": "muu7DisY3HdC"
   },
   "source": [
    "### 방법 2) np.random.choice 에서 sampling\n",
    "\n",
    "- np.random.choice 의 `parameter p`에 softmax 확률 분포 지정하여 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796166df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c335e829",
   "metadata": {
    "id": "13-ePWwd3HdD"
   },
   "source": [
    "### REINFORCE 구현을  위해  total expected return $G_t$ 를 estimate 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f21683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 step 만에 spisode 종료 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3f314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41687178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the array direction for cumsum and then\n",
    "# revert back to the original order\n",
    "# return r - r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodic task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuing task\n",
    "def discount_rewards(rewards):\n",
    "    # cumsum의 배열 방향을 반대로 한 다음 원래 순서로 되돌립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84672a7",
   "metadata": {
    "id": "simlDDmD3HdE"
   },
   "source": [
    "### REINFORCE 구현을 위한 Score Function\n",
    "\n",
    "- 확률 밀도 함수가 매개 변수와 관련하여 미분할 수있는 경우 REINFORCE를 구현하려면 sample () 및 log_prob () 만 필요\n",
    "\n",
    "$$\\Delta_{\\theta} = \\alpha r \\frac{\\partial log p(a | \\pi^{\\theta}(s))}{\\partial\\theta}$$  \n",
    "\n",
    "$\\alpha$ - learning rate, r - reward,  $p(a|\\pi^\\theta(s))$ - probability of taking action a  \n",
    "\n",
    "\n",
    "- Network 출력에서 action을 샘플링하고 이 action을 environment에 적용한 다음 log_prob를 사용하여 동등한 손실 함수를 구성.   \n",
    "- optimizer는 경사 하강법을 사용하기 때문에 음수를 사용하는 반면 위의 규칙은 경사 상승을 가정.   \n",
    "- Categorical Policy를 사용하는 경우 REINFORCE를 구현하는 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs = policy_network(state)\n",
    "#loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325fbfe",
   "metadata": {
    "id": "4ZhEg-H53HdE"
   },
   "source": [
    "## Huber Loss\n",
    "\n",
    "- Actor-Critic 의 critic value function 의 loss 계산에 사용  \n",
    "- Huber Loss는 L1과 L2의 장점을 취하면서 단점을 보완하기 위해서 제안된 것이 Huber Loss다.\n",
    "    - 모든 지점에서 미분이 가능하다.  \n",
    "    - Outlier에 상대적으로 Robust하다.\n",
    "<img src=https://bekaykang.github.io/assets/img/post/201209-2.png width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe998ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4c33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22baa1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
