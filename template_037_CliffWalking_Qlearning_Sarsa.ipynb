{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2134c7",
   "metadata": {
    "id": "bb6jLALfGu_q"
   },
   "source": [
    "# Cliff Walking Q-Learning vs Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a4bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c77431a",
   "metadata": {
    "id": "Lm3QoadaGu_x"
   },
   "source": [
    "## Cliff GridWorld 환경 \n",
    "\n",
    "GridWorld라는 클래스를 정의하여 GridWorld 환경을 구현합니다. \n",
    "\n",
    "플레이어의 행동에 따라 상태를 업데이트합니다.\n",
    "\n",
    "UP, DOWN, RIGHT, LEFT 행동에 따라 플레이어의 위치를 변경합니다.\n",
    "플레이어가 절벽에 도달하면 -100의 보상을 받고 에피소드가 종료됩니다.\n",
    "플레이어가 목표에 도달하면 0의 보상을 받고 에피소드가 종료됩니다.\n",
    "그 외의 경우에는 -1의 보상을 받고 에피소드는 계속됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    # 지형의 색상을 정의한 딕셔너리\n",
    "    def __init__(self):\n",
    "        # 그리드를 초기화하여 모든 셀을 일반 지형으로 설정\n",
    "        # 플레이어의 초기 위치를 설정\n",
    "        # 절벽 위치를 설정\n",
    "        # 목표 위치를 설정\n",
    "        # 그리드를 그리는 함수 호출\n",
    "    def _draw_grid(self):\n",
    "        # 그리드를 시각화하기 위한 설정\n",
    "            # 그리드 셀마다 텍스트 추가 (인덱스 및 위치)\n",
    "        # 그리드를 이미지로 표시\n",
    "    def show_grid(self, Q):\n",
    "        # Q 값과 함께 그리드를 시각화하기 위한 설정\n",
    "            # 그리드 셀마다 Q 값 추가\n",
    "        # 그리드를 이미지로 표시\n",
    "    def _id_to_position(self, idx):\n",
    "        # 인덱스를 (y, x) 좌표로 변환\n",
    "    def reset(self):\n",
    "        # 플레이어를 초기 위치로 재설정\n",
    "    def _position_to_id(self, pos):\n",
    "    def step(self, action):\n",
    "        # 행동에 따른 플레이어의 이동 (UP = 0, DOWN = 1, RIGHT = 2, LEFT = 3)\n",
    "        # 규칙 적용\n",
    "            # 플레이어가 절벽에 도달하면 -100 보상과 함께 에피소드 종료\n",
    "            # 플레이어가 목표에 도달하면 0 보상과 함께 에피소드 종료\n",
    "            # 그 외의 경우에는 -1 보상과 함께 에피소드 계속\n",
    "        # 플레이어의 새로운 위치의 인덱스, 보상, 에피소드 종료 여부를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aef69a",
   "metadata": {
    "id": "5pTV7zRTak7w"
   },
   "source": [
    "- environment 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643ff0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62f0c6b0",
   "metadata": {
    "id": "bCOUqkHuaqod"
   },
   "source": [
    "- action space 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee0905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c567cd4c",
   "metadata": {
    "id": "HCogV92nauY8"
   },
   "source": [
    "- state 수, action 수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed6c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c729b74e",
   "metadata": {
    "id": "ZMbDZYdEa1WT"
   },
   "source": [
    "- 정책 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon-greedy policy\n",
    "def pi(Q, state, e=0.1):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05f91c",
   "metadata": {
    "id": "bEdf77k1a5H-"
   },
   "source": [
    "### Q-Learning, Sarsa 알고리즘 정의\n",
    "\n",
    "- Q-Learning에서는 `np.max(Q[s_])`를 사용하여 다음 상태에서 가능한 행동 중 최대 Q 값을 선택합니다.\n",
    "- Sarsa에서는 `pi(Q, s_, epsilon)`를 사용하여 다음 상태에서 실제로 선택된 행동 a_의 Q 값을 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef83f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에피소드 수, 할인 인자, 학습률, 탐색률 설정\n",
    "# Q-Learning 알고리즘 구현\n",
    "def Q_Learning():\n",
    "            # 행동 선택\n",
    "            # Q 값 업데이트\n",
    "# Sarsa 알고리즘 구현\n",
    "def Sarsa():\n",
    "        # 행동 선택\n",
    "            # 다음 행동 선택\n",
    "            # Q 값 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1aee0",
   "metadata": {
    "id": "ZZLEFZvpbArh"
   },
   "source": [
    "- Q-Learning state-action value table 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e946e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d035bf3c",
   "metadata": {
    "id": "Bfs-gyYCbOAg"
   },
   "source": [
    "- Sarsa state-action value table 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7029543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c76ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "937_CliffWalking_Qlearning_Sarsa.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "38b01fad805e8a05966ae056795aab5a88705fcc8f2ad39b88ffa64085cc8280"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
