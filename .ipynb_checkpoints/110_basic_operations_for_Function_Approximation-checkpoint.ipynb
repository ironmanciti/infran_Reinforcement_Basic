{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTfA3OUb3Hcx"
   },
   "source": [
    "# 110. Deep Neural Network을 이용한 함수 근사에서 필요한 torch basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXD5j_4_3Hc0",
    "outputId": "e30cc313-8249-44fa-dc7b-f5cd0520f953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import collections\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "env = gym.make('CartPole-v1')  \n",
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uciS0nsN3Hc2"
   },
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hyL29xvt3Hc3"
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "\n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mBA3O2s3Hc4",
    "outputId": "e298887e-22c3-4f9c-a871-83c697938130"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34052\\665540225.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0ms_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mexperience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExperience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "#Initialize replay memory D to capacity N\n",
    "D = ExperienceReplay(10)\n",
    "\n",
    "s = env.reset()\n",
    "for i in range(10):\n",
    "    a = env.action_space.sample()\n",
    "    s_, r, truncated, _ = env.step(a)\n",
    "    experience = Experience(s, a, r, done, s_)\n",
    "    D.add(experience)\n",
    "    s = s_\n",
    "    \n",
    "D.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBWTiu01BTOB"
   },
   "source": [
    "## Sample random minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8t4tHzM6BO4_",
    "outputId": "74d7419c-16bd-4254-8ded-71c8b88b8f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4) (5,) (5,) (5, 4) (5,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.02142895,  0.53795326,  0.00531706, -0.81063944],\n",
       "        [ 0.04886325,  0.5386855 , -0.03628989, -0.8270482 ],\n",
       "        [ 0.04199904,  0.3432107 , -0.02576269, -0.5263599 ],\n",
       "        [ 0.01736601, -0.24154095,  0.01275495,  0.3382676 ],\n",
       "        [ 0.01456782,  0.34305686,  0.01577644, -0.52296937]],\n",
       "       dtype=float32),\n",
       " array([0, 1, 1, 1, 1], dtype=int64),\n",
       " array([1., 1., 1., 1., 1.], dtype=float32),\n",
       " array([[ 0.03218802,  0.34275886, -0.01089573, -0.5162888 ],\n",
       "        [ 0.05963696,  0.7342844 , -0.05283085, -1.1309203 ],\n",
       "        [ 0.04886325,  0.5386855 , -0.03628989, -0.8270482 ],\n",
       "        [ 0.0125352 , -0.04660281,  0.0195203 ,  0.04963399],\n",
       "        [ 0.02142895,  0.53795326,  0.00531706, -0.81063944]],\n",
       "       dtype=float32),\n",
       " array([False, False, False, False, False]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states, actions, rewards, dones, next_states = D.sample(5)\n",
    "\n",
    "print(states.shape, actions.shape, rewards.shape, next_states.shape, dones.shape)\n",
    "states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-xKKrqw3Hc5"
   },
   "source": [
    "## Select Action \n",
    "\n",
    "- state가 4 개의 feature로 구성되고 각 state에서의 action이 2 가지인 MDP의 parameter화 된 state action value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gbVFe-jz3Hc6"
   },
   "outputs": [],
   "source": [
    "n_inputs = 4  # state feature\n",
    "n_outputs = 2  # action space\n",
    "hidden_layer = 64\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_inputs, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = torch.relu(self.linear1(x))\n",
    "        output = self.linear2(a1)\n",
    "        return output\n",
    "\n",
    "Q = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0fir5Pl3Hc7"
   },
   "source": [
    "- 입력 : 4 개 feature 로 구성된 state \n",
    "- 출력 : 2 개 action values  \n",
    "\n",
    "- greedy action : $max_{a'}Q(s', a';\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQ1VsEkd3Hc7",
    "outputId": "c08766e7-27a2-4d04-f399-c3fd1d5e0584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2143, 0.2288], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "\n",
    "action_values = Q(torch.tensor(s).to(device))\n",
    "action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTXrKPAk3Hc8",
    "outputId": "ac2fd21b-945a-4ce1-f18c-31ecb8971e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greedy action\n",
    "action = torch.argmax(action_values).item() \n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dILVsp1p3Hc9"
   },
   "source": [
    "## State-Action Value (q value) from DQN \n",
    "\n",
    "Q-network 에서 입력으로 주어진 states 에 대응하는 action values 를 출력으로 얻어 greedy action 을 선택하는 code.  \n",
    "\n",
    "함수 max()는 최대값과 해당 값의 인덱스를 모두 반환하므로 최대값과 argmax를 모두 계산합니다. 이 경우 값에만 관심이 있기 때문에 결과의 첫 번째 항목(values)을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NfWJLj63Hc-",
    "outputId": "8b454cf3-5f34-4100-ce6e-a503e8173de9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2594, 0.3157],\n",
      "        [0.2572, 0.3212],\n",
      "        [0.2460, 0.2823],\n",
      "        [0.1780, 0.2676],\n",
      "        [0.2470, 0.2801]])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.3157, 0.3212, 0.2823, 0.2676, 0.2801]),\n",
      "indices=tensor([1, 1, 1, 1, 1]))\n",
      "\n",
      "tensor([0.3157, 0.3212, 0.2823, 0.2676, 0.2801])\n",
      "tensor([1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "states_v = torch.tensor(states).to(device)\n",
    "action_values = Q(states_v).detach().cpu()\n",
    "\n",
    "print(action_values)\n",
    "print(torch.max(action_values, dim=1))\n",
    "print()\n",
    "\n",
    "values, indices = torch.max(action_values, dim=1)\n",
    "\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSugiYLx3Hc-"
   },
   "source": [
    "## torch.gather\n",
    "\n",
    "- torch.gather 함수 (또는 torch.Tensor.gather)는 다중 인덱스 선택 방법  \n",
    "\n",
    "- 첫 번째 인수인 input은 요소를 선택하려는 소스 텐서. 두 번째 dim은 수집하려는 차원. 마지막으로 index는 입력을 인덱싱하는 인덱스.\n",
    "\n",
    "4개의 항목과 4개의 작업으로 구성된 일괄 처리가 있는 간단한 예제 사례에서 gather가 수행하는 작업의 요약입니다.\n",
    "\n",
    "```\n",
    "state_action_values = net(states_v).gather(1, actions_v.unsqueeze(1))\n",
    "```\n",
    "\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/1*fS-9p5EBKVgl69Gy0gwjGQ.png width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsSGQnQa3Hc_",
    "outputId": "9c50f027-6290-4904-8017-ee63234e88aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0214,  0.5380,  0.0053, -0.8106],\n",
       "        [ 0.0489,  0.5387, -0.0363, -0.8270],\n",
       "        [ 0.0420,  0.3432, -0.0258, -0.5264],\n",
       "        [ 0.0174, -0.2415,  0.0128,  0.3383],\n",
       "        [ 0.0146,  0.3431,  0.0158, -0.5230]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_v  # 4개의 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNSRn_CT3Hc_",
    "outputId": "583135fa-96bd-4e43-e333-8b3a314e1363"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2594, 0.3157],\n",
       "        [0.2572, 0.3212],\n",
       "        [0.2460, 0.2823],\n",
       "        [0.1780, 0.2676],\n",
       "        [0.2470, 0.2801]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = Q(states_v)\n",
    "q_values  # 2 개의 action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsIuw19X3Hc_",
    "outputId": "aaf625d1-bc81-4d94-be0b-4174fb1892db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = torch.LongTensor([1, 0, 1, 1, 0]).unsqueeze(1).to(device)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXOP_jIq3HdA",
    "outputId": "a58decc9-1adb-4ad1-b6be-e52478af0506"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3157],\n",
       "        [0.2572],\n",
       "        [0.2823],\n",
       "        [0.2676],\n",
       "        [0.2470]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(q_values, 1, action)  #q_value의 axis=1에서 action index 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNvF6oCP3HdA",
    "outputId": "7be184f1-fd48-4dba-d1e3-fa2421c7a1aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3157],\n",
       "        [0.2572],\n",
       "        [0.2823],\n",
       "        [0.2676],\n",
       "        [0.2470]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.gather(1, action)   # 위와 동일 operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufB1JYnC3HdB"
   },
   "source": [
    "## REINFORECE 알고리즘 지원을 위한 PROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS\n",
    "\n",
    "- distribution 패키지에는 매개변수화할 수 있는 확률 분포와 sampling 함수가 포함되어 있습니다. 이를 통해 최적화를 위한 확률적 계산 그래프 및 확률적 기울기 추정기를 구성할 수 있습니다. \n",
    "\n",
    "- torch 는 다음과 같이 REINFORCE 알고리즘을 지원합니다.\n",
    "\n",
    "```python\n",
    "    probs = policy_network(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    next_state, reward = env.step(action)\n",
    "    loss = -m.log_prob(action) * reward\n",
    "    loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEN8t3cq3HdB"
   },
   "source": [
    "### 방법 1) Categorical(probs) 에서 sampling\n",
    "\n",
    "'probs'가 길이가 'K'인 1차원 array인 경우, 각 element 는 해당 인덱스에서 클래스를 샘플링할 상대 확률입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRFyebJE3HdB",
    "outputId": "98223f29-3e0b-4dc3-ecc5-682cb2f477dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax 확률 분포 : tensor([0.1481, 0.3202, 0.2968, 0.2350]), sum = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Categorical(probs: torch.Size([4]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "logits = torch.rand(4)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(f\"softmax 확률 분포 : {probs}, sum = {probs.sum()}\")\n",
    "\n",
    "# 각 class 를 sampling 할 상대 확률\n",
    "m = Categorical(probs)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TACqKr0Y3HdC"
   },
   "source": [
    "위의 m 에서 sampling 을 반복하면 softmax 확률 분포로 sampling 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHvhhzZO3HdC",
    "outputId": "3e2e55c8-868a-4315-e11d-c5cfce92dc4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14536666666666667,\n",
       " 0.32043333333333335,\n",
       " 0.2977666666666667,\n",
       " 0.23643333333333333]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "samples = []\n",
    "\n",
    "for _ in range(30000):\n",
    "    a = m.sample()\n",
    "    samples.append(a.item())\n",
    "\n",
    "[cnt/len(samples) for a, cnt in sorted(Counter(samples).items())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muu7DisY3HdC"
   },
   "source": [
    "### 방법 2) np.random.choice 에서 sampling\n",
    "\n",
    "- np.random.choice 의 `parameter p`에 softmax 확률 분포 지정하여 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewhGZw5w3HdC",
    "outputId": "bc228a97-4870-4343-cae7-645ae0bff82e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14873333333333333,\n",
       " 0.32306666666666667,\n",
       " 0.2950333333333333,\n",
       " 0.23316666666666666]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = []\n",
    "\n",
    "for _ in range(30000):\n",
    "    a = np.random.choice(4, p=probs.numpy())\n",
    "    samples.append(a)\n",
    "    \n",
    "[cnt/len(samples) for a, cnt in sorted(Counter(samples).items())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13-ePWwd3HdD"
   },
   "source": [
    "### REINFORCE 구현을  위해  total expected return $G_t$ 를 estimate 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dA4SgEC23HdD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 5 step 만에 spisode 종료 가정\n",
    "rewards = [1, 2, 3, 4, 5]\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgyPm6P-3HdD",
    "outputId": "5a2c6d26-e9c2-46a1-9d8e-70ce7caad763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.604476049999999 13.741895 11.8605 8.95 5\n"
     ]
    }
   ],
   "source": [
    "G_0 = 1 + 0.99**1 * 2 + 0.99**2 * 3 + 0.99**3 * 4 + 0.99**4 * 5\n",
    "G_1 = 2 + 0.99**1 * 3 + 0.99**2 * 4 + 0.99**3 * 5\n",
    "G_2 = 3 + 0.99**1 * 4 + 0.99**2 * 5\n",
    "G_3 = 4 + 0.99**1 * 5\n",
    "G_4 = 5\n",
    "print(G_0, G_1, G_2, G_3, G_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fcFvCy3I3HdD"
   },
   "outputs": [],
   "source": [
    "r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
    "# Reverse the array direction for cumsum and then\n",
    "# revert back to the original order\n",
    "r = r[::-1].cumsum()[::-1]\n",
    "# return r - r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkFx4vW13HdD",
    "outputId": "e5fff025-cbe8-4d41-a3a7-e25e12393d9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.60447605, 13.741895  , 11.8605    ,  8.95      ,  5.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# episodic task\n",
    "Returns = []\n",
    "G = 0\n",
    "for r in rewards[::-1]:\n",
    "    G = r + gamma * G\n",
    "    Returns.append(G)\n",
    "    \n",
    "Returns = np.array(Returns[::-1], dtype=np.float64)\n",
    "Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlrV5LBC3HdE",
    "outputId": "f4289481-6034-4738-f441-fdb900a3a6e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.60447605 13.741895   11.8605      8.95        5.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.77310184,  2.91052079,  1.02912579, -1.88137421, -5.83137421])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continuing task\n",
    "def discount_rewards(rewards):\n",
    "    Returns = []\n",
    "    G = 0\n",
    "    for r in rewards[::-1]:\n",
    "        G = r + gamma * G\n",
    "        Returns.append(G)\n",
    "    # cumsum의 배열 방향을 반대로 한 다음 원래 순서로 되돌립니다.\n",
    "    Returns = np.array(Returns[::-1], dtype=np.float64)\n",
    "    print(Returns)\n",
    "    return Returns - Returns.mean()\n",
    "\n",
    "discount_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "simlDDmD3HdE"
   },
   "source": [
    "### REINFORCE 구현을 위한 Score Function\n",
    "\n",
    "- 확률 밀도 함수가 매개 변수와 관련하여 미분할 수있는 경우 REINFORCE를 구현하려면 sample () 및 log_prob () 만 필요\n",
    "\n",
    "$$\\Delta_{\\theta} = \\alpha r \\frac{\\partial log p(a | \\pi^{\\theta}(s))}{\\partial\\theta}$$  \n",
    "\n",
    "$\\alpha$ - learning rate, r - reward,  $p(a|\\pi^\\theta(s))$ - probability of taking action a  \n",
    "\n",
    "\n",
    "- Network 출력에서 action을 샘플링하고 이 action을 environment에 적용한 다음 log_prob를 사용하여 동등한 손실 함수를 구성.   \n",
    "- optimizer는 경사 하강법을 사용하기 때문에 음수를 사용하는 반면 위의 규칙은 경사 상승을 가정.   \n",
    "- Categorical Policy를 사용하는 경우 REINFORCE를 구현하는 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhVDiCjk3HdE",
    "outputId": "77da07fc-a3f6-4897-9e19-3f837cf924a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3863)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')  \n",
    "s = env.reset()\n",
    "\n",
    "#probs = policy_network(state)\n",
    "logits = torch.rand(2)\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "m = Categorical(probs)\n",
    "action = m.sample()\n",
    "\n",
    "next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "loss = -m.log_prob(action) * reward\n",
    "#loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZhEg-H53HdE"
   },
   "source": [
    "## Huber Loss\n",
    "\n",
    "- Actor-Critic 의 critic value function 의 loss 계산에 사용  \n",
    "- Huber Loss는 L1과 L2의 장점을 취하면서 단점을 보완하기 위해서 제안된 것이 Huber Loss다.\n",
    "    - 모든 지점에서 미분이 가능하다.  \n",
    "    - Outlier에 상대적으로 Robust하다.\n",
    "<img src=https://bekaykang.github.io/assets/img/post/201209-2.png width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn8JMIvJ3HdF",
    "outputId": "64fbcd18-6ad3-4227-e2ba-3fd8d6eab8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "curr_q = torch.FloatTensor([10,11,12,10,9])\n",
    "target_q = torch.FloatTensor([12,8,10,13,11])\n",
    "\n",
    "loss = F.smooth_l1_loss(curr_q, target_q)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(curr_q, target_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "110_basic_operations_for_Function_Approximation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
